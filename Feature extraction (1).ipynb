{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import VGG16\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"C:/Users/DIBYAJYOTI HALOI/OneDrive/Desktop/summer project/mask folder/Train\"\n",
    "valid_dir=\"C:/Users/DIBYAJYOTI HALOI/OneDrive/Desktop/summer project/mask folder/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights='imagenet', \n",
    "                  include_top=False,\n",
    "                  input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Show architecture\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3103 images belonging to 2 classes.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIBYAJYOTI HALOI\\OneDrive\\Documents\\tempo\\lib\\site-packages\\PIL\\Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 11s 11s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - ETA: 0sFound 1033 images belonging to 2 classes.\n",
      "1/1 [==============================] - 675s 675s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 512))  # Must be equal to the output of the convolutional base\n",
    "    labels = np.zeros(shape=(sample_count,2))\n",
    "    # Preprocess data\n",
    "    generator = datagen.flow_from_directory(directory,\n",
    "                                            target_size=(img_width,img_height),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode='categorical')\n",
    "    # Pass data through convolutional base\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "    \n",
    "train_features, train_labels = extract_features(train_dir, 3103)  # Agree with our small dataset size\n",
    "validation_features, validation_labels = extract_features(valid_dir, 1033)\n",
    "# test_features, test_labels = extract_features(test_dir, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,026\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GlobalAveragePooling2D(input_shape=(7,7,512)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.6626\n",
      "Epoch 1: val_loss improved from inf to 0.53318, saving model to model-001-0.667741-0.849952.h5\n",
      "97/97 [==============================] - 1s 6ms/step - loss: 0.6650 - acc: 0.6677 - val_loss: 0.5332 - val_acc: 0.8500\n",
      "Epoch 2/150\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 0.4680 - acc: 0.8736\n",
      "Epoch 2: val_loss improved from 0.53318 to 0.41700, saving model to model-002-0.872059-0.891578.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.4664 - acc: 0.8721 - val_loss: 0.4170 - val_acc: 0.8916\n",
      "Epoch 3/150\n",
      "95/97 [============================>.] - ETA: 0s - loss: 0.3830 - acc: 0.8868\n",
      "Epoch 3: val_loss improved from 0.41700 to 0.36442, saving model to model-003-0.886884-0.881897.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.3822 - acc: 0.8869 - val_loss: 0.3644 - val_acc: 0.8819\n",
      "Epoch 4/150\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 0.3384 - acc: 0.8967\n",
      "Epoch 4: val_loss improved from 0.36442 to 0.31698, saving model to model-004-0.898485-0.912875.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.3350 - acc: 0.8985 - val_loss: 0.3170 - val_acc: 0.9129\n",
      "Epoch 5/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.3054 - acc: 0.9062\n",
      "Epoch 5: val_loss improved from 0.31698 to 0.29052, saving model to model-005-0.905575-0.907067.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.3032 - acc: 0.9056 - val_loss: 0.2905 - val_acc: 0.9071\n",
      "Epoch 6/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9096\n",
      "Epoch 6: val_loss improved from 0.29052 to 0.27673, saving model to model-006-0.910087-0.906099.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.2810 - acc: 0.9101 - val_loss: 0.2767 - val_acc: 0.9061\n",
      "Epoch 7/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.9179\n",
      "Epoch 7: val_loss improved from 0.27673 to 0.25390, saving model to model-007-0.916210-0.914811.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.2632 - acc: 0.9162 - val_loss: 0.2539 - val_acc: 0.9148\n",
      "Epoch 8/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9152\n",
      "Epoch 8: val_loss improved from 0.25390 to 0.24583, saving model to model-008-0.915243-0.914811.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.2480 - acc: 0.9152 - val_loss: 0.2458 - val_acc: 0.9148\n",
      "Epoch 9/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.2338 - acc: 0.9279\n",
      "Epoch 9: val_loss improved from 0.24583 to 0.24154, saving model to model-009-0.926200-0.917715.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2353 - acc: 0.9262 - val_loss: 0.2415 - val_acc: 0.9177\n",
      "Epoch 10/150\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 0.2237 - acc: 0.9290\n",
      "Epoch 10: val_loss improved from 0.24154 to 0.22297, saving model to model-010-0.926845-0.920620.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.2268 - acc: 0.9268 - val_loss: 0.2230 - val_acc: 0.9206\n",
      "Epoch 11/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.2180 - acc: 0.9261\n",
      "Epoch 11: val_loss improved from 0.22297 to 0.21332, saving model to model-011-0.927490-0.926428.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2163 - acc: 0.9275 - val_loss: 0.2133 - val_acc: 0.9264\n",
      "Epoch 12/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.2078 - acc: 0.9324\n",
      "Epoch 12: val_loss improved from 0.21332 to 0.21124, saving model to model-012-0.931679-0.924492.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2087 - acc: 0.9317 - val_loss: 0.2112 - val_acc: 0.9245\n",
      "Epoch 13/150\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 0.2052 - acc: 0.9358\n",
      "Epoch 13: val_loss improved from 0.21124 to 0.20294, saving model to model-013-0.936835-0.925460.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.2023 - acc: 0.9368 - val_loss: 0.2029 - val_acc: 0.9255\n",
      "Epoch 14/150\n",
      "82/97 [========================>.....] - ETA: 0s - loss: 0.1954 - acc: 0.9386\n",
      "Epoch 14: val_loss improved from 0.20294 to 0.19447, saving model to model-014-0.938124-0.928364.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1954 - acc: 0.9381 - val_loss: 0.1945 - val_acc: 0.9284\n",
      "Epoch 15/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1898 - acc: 0.9389\n",
      "Epoch 15: val_loss did not improve from 0.19447\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1896 - acc: 0.9401 - val_loss: 0.1964 - val_acc: 0.9264\n",
      "Epoch 16/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.1840 - acc: 0.9429\n",
      "Epoch 16: val_loss improved from 0.19447 to 0.18939, saving model to model-016-0.942314-0.927396.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1851 - acc: 0.9423 - val_loss: 0.1894 - val_acc: 0.9274\n",
      "Epoch 17/150\n",
      "66/97 [===================>..........] - ETA: 0s - loss: 0.1729 - acc: 0.9470\n",
      "Epoch 17: val_loss improved from 0.18939 to 0.18574, saving model to model-017-0.943603-0.928364.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1795 - acc: 0.9436 - val_loss: 0.1857 - val_acc: 0.9284\n",
      "Epoch 18/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.1742 - acc: 0.9451\n",
      "Epoch 18: val_loss improved from 0.18574 to 0.17856, saving model to model-018-0.944247-0.934172.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1752 - acc: 0.9442 - val_loss: 0.1786 - val_acc: 0.9342\n",
      "Epoch 19/150\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 0.1701 - acc: 0.9474\n",
      "Epoch 19: val_loss did not improve from 0.17856\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1710 - acc: 0.9459 - val_loss: 0.1888 - val_acc: 0.9226\n",
      "Epoch 20/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.1710 - acc: 0.9414\n",
      "Epoch 20: val_loss improved from 0.17856 to 0.17346, saving model to model-020-0.943925-0.935140.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1682 - acc: 0.9439 - val_loss: 0.1735 - val_acc: 0.9351\n",
      "Epoch 21/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.1636 - acc: 0.9492\n",
      "Epoch 21: val_loss improved from 0.17346 to 0.16957, saving model to model-021-0.949404-0.936108.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1633 - acc: 0.9494 - val_loss: 0.1696 - val_acc: 0.9361\n",
      "Epoch 22/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.1598 - acc: 0.9509\n",
      "Epoch 22: val_loss improved from 0.16957 to 0.16164, saving model to model-022-0.951015-0.941917.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1599 - acc: 0.9510 - val_loss: 0.1616 - val_acc: 0.9419\n",
      "Epoch 23/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1593 - acc: 0.9508\n",
      "Epoch 23: val_loss did not improve from 0.16164\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1576 - acc: 0.9510 - val_loss: 0.1694 - val_acc: 0.9313\n",
      "Epoch 24/150\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 0.1545 - acc: 0.9531\n",
      "Epoch 24: val_loss improved from 0.16164 to 0.16149, saving model to model-024-0.952304-0.938045.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1540 - acc: 0.9523 - val_loss: 0.1615 - val_acc: 0.9380\n",
      "Epoch 25/150\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 0.1495 - acc: 0.9524\n",
      "Epoch 25: val_loss improved from 0.16149 to 0.15643, saving model to model-025-0.952626-0.942885.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1520 - acc: 0.9526 - val_loss: 0.1564 - val_acc: 0.9429\n",
      "Epoch 26/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.1471 - acc: 0.9553\n",
      "Epoch 26: val_loss improved from 0.15643 to 0.15481, saving model to model-026-0.953916-0.942885.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1488 - acc: 0.9539 - val_loss: 0.1548 - val_acc: 0.9429\n",
      "Epoch 27/150\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 0.1471 - acc: 0.9531\n",
      "Epoch 27: val_loss improved from 0.15481 to 0.14961, saving model to model-027-0.953271-0.946757.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1464 - acc: 0.9533 - val_loss: 0.1496 - val_acc: 0.9468\n",
      "Epoch 28/150\n",
      "85/97 [=========================>....] - ETA: 0s - loss: 0.1438 - acc: 0.9551\n",
      "Epoch 28: val_loss improved from 0.14961 to 0.14783, saving model to model-028-0.956171-0.946757.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1437 - acc: 0.9562 - val_loss: 0.1478 - val_acc: 0.9468\n",
      "Epoch 29/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.1406 - acc: 0.9570\n",
      "Epoch 29: val_loss did not improve from 0.14783\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1418 - acc: 0.9562 - val_loss: 0.1543 - val_acc: 0.9361\n",
      "Epoch 30/150\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 0.1392 - acc: 0.9576\n",
      "Epoch 30: val_loss improved from 0.14783 to 0.14319, saving model to model-030-0.956816-0.948693.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1407 - acc: 0.9568 - val_loss: 0.1432 - val_acc: 0.9487\n",
      "Epoch 31/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.1347 - acc: 0.9582\n",
      "Epoch 31: val_loss did not improve from 0.14319\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1376 - acc: 0.9575 - val_loss: 0.1473 - val_acc: 0.9409\n",
      "Epoch 32/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.1378 - acc: 0.9575\n",
      "Epoch 32: val_loss improved from 0.14319 to 0.13908, saving model to model-032-0.957461-0.949661.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1376 - acc: 0.9575 - val_loss: 0.1391 - val_acc: 0.9497\n",
      "Epoch 33/150\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 0.1345 - acc: 0.9580\n",
      "Epoch 33: val_loss improved from 0.13908 to 0.13688, saving model to model-033-0.957783-0.949661.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1344 - acc: 0.9578 - val_loss: 0.1369 - val_acc: 0.9497\n",
      "Epoch 34/150\n",
      "85/97 [=========================>....] - ETA: 0s - loss: 0.1364 - acc: 0.9555\n",
      "Epoch 34: val_loss improved from 0.13688 to 0.13651, saving model to model-034-0.958105-0.948693.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1331 - acc: 0.9581 - val_loss: 0.1365 - val_acc: 0.9487\n",
      "Epoch 35/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.1309 - acc: 0.9588\n",
      "Epoch 35: val_loss did not improve from 0.13651\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1315 - acc: 0.9581 - val_loss: 0.1378 - val_acc: 0.9468\n",
      "Epoch 36/150\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 0.1320 - acc: 0.9568\n",
      "Epoch 36: val_loss improved from 0.13651 to 0.13329, saving model to model-036-0.957783-0.948693.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1305 - acc: 0.9578 - val_loss: 0.1333 - val_acc: 0.9487\n",
      "Epoch 37/150\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 0.1281 - acc: 0.9578\n",
      "Epoch 37: val_loss did not improve from 0.13329\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1277 - acc: 0.9581 - val_loss: 0.1366 - val_acc: 0.9458\n",
      "Epoch 38/150\n",
      "95/97 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9592\n",
      "Epoch 38: val_loss did not improve from 0.13329\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1269 - acc: 0.9594 - val_loss: 0.1375 - val_acc: 0.9448\n",
      "Epoch 39/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.1227 - acc: 0.9639\n",
      "Epoch 39: val_loss did not improve from 0.13329\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1248 - acc: 0.9597 - val_loss: 0.1437 - val_acc: 0.9380\n",
      "Epoch 40/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.1224 - acc: 0.9601\n",
      "Epoch 40: val_loss did not improve from 0.13329\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1237 - acc: 0.9604 - val_loss: 0.1357 - val_acc: 0.9448\n",
      "Epoch 41/150\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 0.1210 - acc: 0.9620\n",
      "Epoch 41: val_loss improved from 0.13329 to 0.12789, saving model to model-041-0.961972-0.947725.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1221 - acc: 0.9620 - val_loss: 0.1279 - val_acc: 0.9477\n",
      "Epoch 42/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.1158 - acc: 0.9635\n",
      "Epoch 42: val_loss did not improve from 0.12789\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1218 - acc: 0.9594 - val_loss: 0.1392 - val_acc: 0.9409\n",
      "Epoch 43/150\n",
      "88/97 [==========================>...] - ETA: 0s - loss: 0.1199 - acc: 0.9624\n",
      "Epoch 43: val_loss improved from 0.12789 to 0.12497, saving model to model-043-0.961972-0.946757.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1192 - acc: 0.9620 - val_loss: 0.1250 - val_acc: 0.9468\n",
      "Epoch 44/150\n",
      "86/97 [=========================>....] - ETA: 0s - loss: 0.1188 - acc: 0.9611\n",
      "Epoch 44: val_loss did not improve from 0.12497\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1196 - acc: 0.9604 - val_loss: 0.1363 - val_acc: 0.9439\n",
      "Epoch 45/150\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 0.1163 - acc: 0.9623\n",
      "Epoch 45: val_loss did not improve from 0.12497\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - acc: 0.9617 - val_loss: 0.1251 - val_acc: 0.9516\n",
      "Epoch 46/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.1139 - acc: 0.9641\n",
      "Epoch 46: val_loss did not improve from 0.12497\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1169 - acc: 0.9629 - val_loss: 0.1252 - val_acc: 0.9506\n",
      "Epoch 47/150\n",
      "84/97 [========================>.....] - ETA: 0s - loss: 0.1178 - acc: 0.9598\n",
      "Epoch 47: val_loss improved from 0.12497 to 0.12401, saving model to model-047-0.961650-0.952565.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1152 - acc: 0.9617 - val_loss: 0.1240 - val_acc: 0.9526\n",
      "Epoch 48/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9611\n",
      "Epoch 48: val_loss improved from 0.12401 to 0.12346, saving model to model-048-0.961972-0.951597.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1149 - acc: 0.9620 - val_loss: 0.1235 - val_acc: 0.9516\n",
      "Epoch 49/150\n",
      "82/97 [========================>.....] - ETA: 0s - loss: 0.1139 - acc: 0.9611\n",
      "Epoch 49: val_loss did not improve from 0.12346\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1139 - acc: 0.9617 - val_loss: 0.1328 - val_acc: 0.9448\n",
      "Epoch 50/150\n",
      "85/97 [=========================>....] - ETA: 0s - loss: 0.1105 - acc: 0.9618\n",
      "Epoch 50: val_loss improved from 0.12346 to 0.12179, saving model to model-050-0.962295-0.950629.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1132 - acc: 0.9623 - val_loss: 0.1218 - val_acc: 0.9506\n",
      "Epoch 51/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.1115 - acc: 0.9657\n",
      "Epoch 51: val_loss did not improve from 0.12179\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1125 - acc: 0.9652 - val_loss: 0.1266 - val_acc: 0.9487\n",
      "Epoch 52/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.1104 - acc: 0.9619\n",
      "Epoch 52: val_loss did not improve from 0.12179\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1105 - acc: 0.9629 - val_loss: 0.1279 - val_acc: 0.9468\n",
      "Epoch 53/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.1119 - acc: 0.9646\n",
      "Epoch 53: val_loss did not improve from 0.12179\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1108 - acc: 0.9633 - val_loss: 0.1327 - val_acc: 0.9419\n",
      "Epoch 54/150\n",
      "81/97 [========================>.....] - ETA: 0s - loss: 0.1031 - acc: 0.9676\n",
      "Epoch 54: val_loss improved from 0.12179 to 0.12068, saving model to model-054-0.964550-0.950629.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1098 - acc: 0.9646 - val_loss: 0.1207 - val_acc: 0.9506\n",
      "Epoch 55/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1100 - acc: 0.9645\n",
      "Epoch 55: val_loss did not improve from 0.12068\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1086 - acc: 0.9649 - val_loss: 0.1212 - val_acc: 0.9497\n",
      "Epoch 56/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1054 - acc: 0.9659\n",
      "Epoch 56: val_loss improved from 0.12068 to 0.12051, saving model to model-056-0.965195-0.949661.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1072 - acc: 0.9652 - val_loss: 0.1205 - val_acc: 0.9497\n",
      "Epoch 57/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.1056 - acc: 0.9668\n",
      "Epoch 57: val_loss improved from 0.12051 to 0.11550, saving model to model-057-0.965840-0.956438.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1080 - acc: 0.9658 - val_loss: 0.1155 - val_acc: 0.9564\n",
      "Epoch 58/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.1086 - acc: 0.9648\n",
      "Epoch 58: val_loss improved from 0.11550 to 0.11522, saving model to model-058-0.966484-0.957406.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1057 - acc: 0.9665 - val_loss: 0.1152 - val_acc: 0.9574\n",
      "Epoch 59/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.1055 - acc: 0.9646\n",
      "Epoch 59: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1055 - acc: 0.9646 - val_loss: 0.1157 - val_acc: 0.9535\n",
      "Epoch 60/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1033 - acc: 0.9663\n",
      "Epoch 60: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1041 - acc: 0.9655 - val_loss: 0.1161 - val_acc: 0.9535\n",
      "Epoch 61/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.1060 - acc: 0.9658\n",
      "Epoch 61: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1036 - acc: 0.9662 - val_loss: 0.1162 - val_acc: 0.9526\n",
      "Epoch 62/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.1025 - acc: 0.9633\n",
      "Epoch 62: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1031 - acc: 0.9649 - val_loss: 0.1178 - val_acc: 0.9506\n",
      "Epoch 63/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.1044 - acc: 0.9651\n",
      "Epoch 63: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1022 - acc: 0.9658 - val_loss: 0.1184 - val_acc: 0.9506\n",
      "Epoch 64/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.1016 - acc: 0.9677\n",
      "Epoch 64: val_loss did not improve from 0.11522\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1016 - acc: 0.9675 - val_loss: 0.1175 - val_acc: 0.9506\n",
      "Epoch 65/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.1022 - acc: 0.9625\n",
      "Epoch 65: val_loss improved from 0.11522 to 0.11031, saving model to model-065-0.965517-0.956438.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1013 - acc: 0.9655 - val_loss: 0.1103 - val_acc: 0.9564\n",
      "Epoch 66/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.1008 - acc: 0.9666\n",
      "Epoch 66: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1007 - acc: 0.9671 - val_loss: 0.1182 - val_acc: 0.9506\n",
      "Epoch 67/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.0986 - acc: 0.9688\n",
      "Epoch 67: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0994 - acc: 0.9687 - val_loss: 0.1169 - val_acc: 0.9506\n",
      "Epoch 68/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0965 - acc: 0.9696\n",
      "Epoch 68: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1005 - acc: 0.9681 - val_loss: 0.1108 - val_acc: 0.9564\n",
      "Epoch 69/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.1013 - acc: 0.9643\n",
      "Epoch 69: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0990 - acc: 0.9658 - val_loss: 0.1108 - val_acc: 0.9574\n",
      "Epoch 70/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.0937 - acc: 0.9705\n",
      "Epoch 70: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0983 - acc: 0.9678 - val_loss: 0.1232 - val_acc: 0.9439\n",
      "Epoch 71/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.0983 - acc: 0.9687\n",
      "Epoch 71: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0983 - acc: 0.9687 - val_loss: 0.1172 - val_acc: 0.9487\n",
      "Epoch 72/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.1009 - acc: 0.9658\n",
      "Epoch 72: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0971 - acc: 0.9681 - val_loss: 0.1110 - val_acc: 0.9564\n",
      "Epoch 73/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0974 - acc: 0.9688\n",
      "Epoch 73: val_loss did not improve from 0.11031\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0962 - acc: 0.9681 - val_loss: 0.1160 - val_acc: 0.9506\n",
      "Epoch 74/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9665\n",
      "Epoch 74: val_loss improved from 0.11031 to 0.10972, saving model to model-074-0.966806-0.957406.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0960 - acc: 0.9668 - val_loss: 0.1097 - val_acc: 0.9574\n",
      "Epoch 75/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0970 - acc: 0.9675\n",
      "Epoch 75: val_loss did not improve from 0.10972\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0954 - acc: 0.9681 - val_loss: 0.1150 - val_acc: 0.9497\n",
      "Epoch 76/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.0949 - acc: 0.9697\n",
      "Epoch 76: val_loss did not improve from 0.10972\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0949 - acc: 0.9697 - val_loss: 0.1118 - val_acc: 0.9555\n",
      "Epoch 77/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0931 - acc: 0.9688\n",
      "Epoch 77: val_loss did not improve from 0.10972\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0943 - acc: 0.9697 - val_loss: 0.1220 - val_acc: 0.9468\n",
      "Epoch 78/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.0956 - acc: 0.9688\n",
      "Epoch 78: val_loss did not improve from 0.10972\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0937 - acc: 0.9684 - val_loss: 0.1129 - val_acc: 0.9545\n",
      "Epoch 79/150\n",
      "79/97 [=======================>......] - ETA: 0s - loss: 0.0951 - acc: 0.9688\n",
      "Epoch 79: val_loss improved from 0.10972 to 0.10902, saving model to model-079-0.969707-0.956438.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.0936 - acc: 0.9697 - val_loss: 0.1090 - val_acc: 0.9564\n",
      "Epoch 80/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0899 - acc: 0.9704\n",
      "Epoch 80: val_loss did not improve from 0.10902\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0931 - acc: 0.9700 - val_loss: 0.1150 - val_acc: 0.9506\n",
      "Epoch 81/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0927 - acc: 0.9704\n",
      "Epoch 81: val_loss did not improve from 0.10902\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0921 - acc: 0.9694 - val_loss: 0.1158 - val_acc: 0.9506\n",
      "Epoch 82/150\n",
      "79/97 [=======================>......] - ETA: 0s - loss: 0.0881 - acc: 0.9707\n",
      "Epoch 82: val_loss did not improve from 0.10902\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0929 - acc: 0.9687 - val_loss: 0.1112 - val_acc: 0.9555\n",
      "Epoch 83/150\n",
      "85/97 [=========================>....] - ETA: 0s - loss: 0.0952 - acc: 0.9673\n",
      "Epoch 83: val_loss improved from 0.10902 to 0.10833, saving model to model-083-0.968740-0.956438.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.0922 - acc: 0.9687 - val_loss: 0.1083 - val_acc: 0.9564\n",
      "Epoch 84/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.0916 - acc: 0.9707\n",
      "Epoch 84: val_loss improved from 0.10833 to 0.10654, saving model to model-084-0.970674-0.956438.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0916 - acc: 0.9707 - val_loss: 0.1065 - val_acc: 0.9564\n",
      "Epoch 85/150\n",
      "81/97 [========================>.....] - ETA: 0s - loss: 0.0916 - acc: 0.9715\n",
      "Epoch 85: val_loss improved from 0.10654 to 0.10600, saving model to model-085-0.972607-0.957406.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0907 - acc: 0.9726 - val_loss: 0.1060 - val_acc: 0.9574\n",
      "Epoch 86/150\n",
      "83/97 [========================>.....] - ETA: 0s - loss: 0.0900 - acc: 0.9703\n",
      "Epoch 86: val_loss improved from 0.10600 to 0.10445, saving model to model-086-0.970996-0.958374.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0906 - acc: 0.9710 - val_loss: 0.1045 - val_acc: 0.9584\n",
      "Epoch 87/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0919 - acc: 0.9688\n",
      "Epoch 87: val_loss improved from 0.10445 to 0.10389, saving model to model-087-0.970351-0.959342.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0897 - acc: 0.9704 - val_loss: 0.1039 - val_acc: 0.9593\n",
      "Epoch 88/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0906 - acc: 0.9697\n",
      "Epoch 88: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0905 - acc: 0.9697 - val_loss: 0.1058 - val_acc: 0.9574\n",
      "Epoch 89/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0866 - acc: 0.9725\n",
      "Epoch 89: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0889 - acc: 0.9700 - val_loss: 0.1089 - val_acc: 0.9555\n",
      "Epoch 90/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0860 - acc: 0.9736\n",
      "Epoch 90: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0885 - acc: 0.9716 - val_loss: 0.1076 - val_acc: 0.9555\n",
      "Epoch 91/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0843 - acc: 0.9749\n",
      "Epoch 91: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0886 - acc: 0.9723 - val_loss: 0.1040 - val_acc: 0.9574\n",
      "Epoch 92/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0863 - acc: 0.9740\n",
      "Epoch 92: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0893 - acc: 0.9713 - val_loss: 0.1102 - val_acc: 0.9555\n",
      "Epoch 93/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0903 - acc: 0.9724\n",
      "Epoch 93: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0873 - acc: 0.9726 - val_loss: 0.1081 - val_acc: 0.9555\n",
      "Epoch 94/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0885 - acc: 0.9704\n",
      "Epoch 94: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0869 - acc: 0.9723 - val_loss: 0.1123 - val_acc: 0.9535\n",
      "Epoch 95/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0865 - acc: 0.9764\n",
      "Epoch 95: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0872 - acc: 0.9752 - val_loss: 0.1110 - val_acc: 0.9564\n",
      "Epoch 96/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.0868 - acc: 0.9707\n",
      "Epoch 96: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0862 - acc: 0.9713 - val_loss: 0.1087 - val_acc: 0.9564\n",
      "Epoch 97/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0866 - acc: 0.9717\n",
      "Epoch 97: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0859 - acc: 0.9716 - val_loss: 0.1041 - val_acc: 0.9564\n",
      "Epoch 98/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.0867 - acc: 0.9724\n",
      "Epoch 98: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0864 - acc: 0.9726 - val_loss: 0.1054 - val_acc: 0.9574\n",
      "Epoch 99/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9711\n",
      "Epoch 99: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0853 - acc: 0.9716 - val_loss: 0.1063 - val_acc: 0.9574\n",
      "Epoch 100/150\n",
      "77/97 [======================>.......] - ETA: 0s - loss: 0.0823 - acc: 0.9728\n",
      "Epoch 100: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0856 - acc: 0.9710 - val_loss: 0.1193 - val_acc: 0.9497\n",
      "Epoch 101/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0904 - acc: 0.9696\n",
      "Epoch 101: val_loss did not improve from 0.10389\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0849 - acc: 0.9729 - val_loss: 0.1091 - val_acc: 0.9564\n",
      "Epoch 102/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9737\n",
      "Epoch 102: val_loss improved from 0.10389 to 0.10332, saving model to model-102-0.973574-0.957406.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0839 - acc: 0.9736 - val_loss: 0.1033 - val_acc: 0.9574\n",
      "Epoch 103/150\n",
      "72/97 [=====================>........] - ETA: 0s - loss: 0.0821 - acc: 0.9727\n",
      "Epoch 103: val_loss improved from 0.10332 to 0.10259, saving model to model-103-0.972285-0.957406.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0840 - acc: 0.9723 - val_loss: 0.1026 - val_acc: 0.9574\n",
      "Epoch 104/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0793 - acc: 0.9747\n",
      "Epoch 104: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0834 - acc: 0.9733 - val_loss: 0.1070 - val_acc: 0.9574\n",
      "Epoch 105/150\n",
      "89/97 [==========================>...] - ETA: 0s - loss: 0.0840 - acc: 0.9723\n",
      "Epoch 105: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0835 - acc: 0.9729 - val_loss: 0.1110 - val_acc: 0.9555\n",
      "Epoch 106/150\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 0.0843 - acc: 0.9726\n",
      "Epoch 106: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0832 - acc: 0.9729 - val_loss: 0.1055 - val_acc: 0.9574\n",
      "Epoch 107/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0804 - acc: 0.9741\n",
      "Epoch 107: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0829 - acc: 0.9729 - val_loss: 0.1058 - val_acc: 0.9574\n",
      "Epoch 108/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9746\n",
      "Epoch 108: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0825 - acc: 0.9749 - val_loss: 0.1115 - val_acc: 0.9535\n",
      "Epoch 109/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0838 - acc: 0.9727\n",
      "Epoch 109: val_loss did not improve from 0.10259\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0840 - acc: 0.9726 - val_loss: 0.1044 - val_acc: 0.9584\n",
      "Epoch 110/150\n",
      "95/97 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9757\n",
      "Epoch 110: val_loss improved from 0.10259 to 0.10061, saving model to model-110-0.975508-0.957406.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0818 - acc: 0.9755 - val_loss: 0.1006 - val_acc: 0.9574\n",
      "Epoch 111/150\n",
      "93/97 [===========================>..] - ETA: 0s - loss: 0.0806 - acc: 0.9761\n",
      "Epoch 111: val_loss improved from 0.10061 to 0.09910, saving model to model-111-0.975830-0.963214.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0818 - acc: 0.9758 - val_loss: 0.0991 - val_acc: 0.9632\n",
      "Epoch 112/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0828 - acc: 0.9742\n",
      "Epoch 112: val_loss did not improve from 0.09910\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0808 - acc: 0.9745 - val_loss: 0.1042 - val_acc: 0.9584\n",
      "Epoch 113/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9747\n",
      "Epoch 113: val_loss did not improve from 0.09910\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0806 - acc: 0.9752 - val_loss: 0.0996 - val_acc: 0.9593\n",
      "Epoch 114/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0844 - acc: 0.9746\n",
      "Epoch 114: val_loss did not improve from 0.09910\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0809 - acc: 0.9758 - val_loss: 0.1090 - val_acc: 0.9555\n",
      "Epoch 115/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0812 - acc: 0.9742\n",
      "Epoch 115: val_loss improved from 0.09910 to 0.09768, saving model to model-115-0.974218-0.963214.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0811 - acc: 0.9742 - val_loss: 0.0977 - val_acc: 0.9632\n",
      "Epoch 116/150\n",
      "94/97 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9751\n",
      "Epoch 116: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0793 - acc: 0.9755 - val_loss: 0.1114 - val_acc: 0.9545\n",
      "Epoch 117/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.0800 - acc: 0.9756\n",
      "Epoch 117: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0791 - acc: 0.9755 - val_loss: 0.0999 - val_acc: 0.9584\n",
      "Epoch 118/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0781 - acc: 0.9758\n",
      "Epoch 118: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0797 - acc: 0.9752 - val_loss: 0.1011 - val_acc: 0.9593\n",
      "Epoch 119/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0767 - acc: 0.9751\n",
      "Epoch 119: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0788 - acc: 0.9752 - val_loss: 0.1027 - val_acc: 0.9593\n",
      "Epoch 120/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0732 - acc: 0.9780\n",
      "Epoch 120: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0792 - acc: 0.9752 - val_loss: 0.1195 - val_acc: 0.9477\n",
      "Epoch 121/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.0791 - acc: 0.9758\n",
      "Epoch 121: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0791 - acc: 0.9758 - val_loss: 0.0986 - val_acc: 0.9613\n",
      "Epoch 122/150\n",
      "82/97 [========================>.....] - ETA: 0s - loss: 0.0807 - acc: 0.9760\n",
      "Epoch 122: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0782 - acc: 0.9771 - val_loss: 0.1030 - val_acc: 0.9584\n",
      "Epoch 123/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0777 - acc: 0.9752\n",
      "Epoch 123: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0779 - acc: 0.9765 - val_loss: 0.1099 - val_acc: 0.9545\n",
      "Epoch 124/150\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.9768\n",
      "Epoch 124: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0780 - acc: 0.9768 - val_loss: 0.1154 - val_acc: 0.9487\n",
      "Epoch 125/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0756 - acc: 0.9776\n",
      "Epoch 125: val_loss did not improve from 0.09768\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0768 - acc: 0.9762 - val_loss: 0.1137 - val_acc: 0.9506\n",
      "Epoch 126/150\n",
      "81/97 [========================>.....] - ETA: 0s - loss: 0.0795 - acc: 0.9745\n",
      "Epoch 126: val_loss improved from 0.09768 to 0.09640, saving model to model-126-0.975508-0.964182.h5\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.0772 - acc: 0.9755 - val_loss: 0.0964 - val_acc: 0.9642\n",
      "Epoch 127/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9769\n",
      "Epoch 127: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0767 - acc: 0.9765 - val_loss: 0.1023 - val_acc: 0.9603\n",
      "Epoch 128/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0765 - acc: 0.9772\n",
      "Epoch 128: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0768 - acc: 0.9765 - val_loss: 0.1033 - val_acc: 0.9593\n",
      "Epoch 129/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.0756 - acc: 0.9777\n",
      "Epoch 129: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0774 - acc: 0.9755 - val_loss: 0.1061 - val_acc: 0.9564\n",
      "Epoch 130/150\n",
      "81/97 [========================>.....] - ETA: 0s - loss: 0.0735 - acc: 0.9796\n",
      "Epoch 130: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0766 - acc: 0.9781 - val_loss: 0.1006 - val_acc: 0.9584\n",
      "Epoch 131/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0747 - acc: 0.9766\n",
      "Epoch 131: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0764 - acc: 0.9755 - val_loss: 0.0997 - val_acc: 0.9584\n",
      "Epoch 132/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0763 - acc: 0.9789\n",
      "Epoch 132: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0753 - acc: 0.9781 - val_loss: 0.1022 - val_acc: 0.9603\n",
      "Epoch 133/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.0784 - acc: 0.9777\n",
      "Epoch 133: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0753 - acc: 0.9781 - val_loss: 0.0996 - val_acc: 0.9584\n",
      "Epoch 134/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0728 - acc: 0.9804\n",
      "Epoch 134: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0750 - acc: 0.9784 - val_loss: 0.1061 - val_acc: 0.9555\n",
      "Epoch 135/150\n",
      "95/97 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9763\n",
      "Epoch 135: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0752 - acc: 0.9762 - val_loss: 0.0968 - val_acc: 0.9622\n",
      "Epoch 136/150\n",
      "72/97 [=====================>........] - ETA: 0s - loss: 0.0716 - acc: 0.9818\n",
      "Epoch 136: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0747 - acc: 0.9787 - val_loss: 0.1034 - val_acc: 0.9593\n",
      "Epoch 137/150\n",
      "79/97 [=======================>......] - ETA: 0s - loss: 0.0770 - acc: 0.9771\n",
      "Epoch 137: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0739 - acc: 0.9791 - val_loss: 0.0975 - val_acc: 0.9622\n",
      "Epoch 138/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0717 - acc: 0.9792\n",
      "Epoch 138: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0741 - acc: 0.9765 - val_loss: 0.1089 - val_acc: 0.9535\n",
      "Epoch 139/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.0724 - acc: 0.9786\n",
      "Epoch 139: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0741 - acc: 0.9781 - val_loss: 0.1053 - val_acc: 0.9555\n",
      "Epoch 140/150\n",
      "73/97 [=====================>........] - ETA: 0s - loss: 0.0756 - acc: 0.9777\n",
      "Epoch 140: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0738 - acc: 0.9781 - val_loss: 0.0983 - val_acc: 0.9613\n",
      "Epoch 141/150\n",
      "76/97 [======================>.......] - ETA: 0s - loss: 0.0739 - acc: 0.9790\n",
      "Epoch 141: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0733 - acc: 0.9797 - val_loss: 0.0976 - val_acc: 0.9622\n",
      "Epoch 142/150\n",
      "74/97 [=====================>........] - ETA: 0s - loss: 0.0787 - acc: 0.9759\n",
      "Epoch 142: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0731 - acc: 0.9791 - val_loss: 0.1019 - val_acc: 0.9574\n",
      "Epoch 143/150\n",
      "91/97 [===========================>..] - ETA: 0s - loss: 0.0733 - acc: 0.9770\n",
      "Epoch 143: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0729 - acc: 0.9771 - val_loss: 0.1027 - val_acc: 0.9564\n",
      "Epoch 144/150\n",
      "75/97 [======================>.......] - ETA: 0s - loss: 0.0705 - acc: 0.9787\n",
      "Epoch 144: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0729 - acc: 0.9794 - val_loss: 0.0998 - val_acc: 0.9574\n",
      "Epoch 145/150\n",
      "96/97 [============================>.] - ETA: 0s - loss: 0.0718 - acc: 0.9792\n",
      "Epoch 145: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0723 - acc: 0.9791 - val_loss: 0.0987 - val_acc: 0.9603\n",
      "Epoch 146/150\n",
      "90/97 [==========================>...] - ETA: 0s - loss: 0.0705 - acc: 0.9812\n",
      "Epoch 146: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0725 - acc: 0.9800 - val_loss: 0.1010 - val_acc: 0.9584\n",
      "Epoch 147/150\n",
      "92/97 [===========================>..] - ETA: 0s - loss: 0.0709 - acc: 0.9793\n",
      "Epoch 147: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0723 - acc: 0.9797 - val_loss: 0.0984 - val_acc: 0.9603\n",
      "Epoch 148/150\n",
      "80/97 [=======================>......] - ETA: 0s - loss: 0.0723 - acc: 0.9785\n",
      "Epoch 148: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0734 - acc: 0.9771 - val_loss: 0.0992 - val_acc: 0.9584\n",
      "Epoch 149/150\n",
      "78/97 [=======================>......] - ETA: 0s - loss: 0.0722 - acc: 0.9796\n",
      "Epoch 149: val_loss did not improve from 0.09640\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0723 - acc: 0.9781 - val_loss: 0.1018 - val_acc: 0.9564\n",
      "Epoch 150/150\n",
      "95/97 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9780\n",
      "Epoch 150: val_loss improved from 0.09640 to 0.09535, saving model to model-150-0.978086-0.964182.h5\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.0713 - acc: 0.9781 - val_loss: 0.0953 - val_acc: 0.9642\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "# Compile model\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint],\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def prediction(img_path):\n",
    "    org_img = image.load_img(img_path)\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_tensor = image.img_to_array(img)  # Image data encoded as integers in the 0–255 range\n",
    "    img_tensor /= 255.  # Normalize to [0,1] for plt.imshow application\n",
    "    plt.imshow(org_img)                           \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Extract features\n",
    "    features = conv_base.predict(img_tensor.reshape(1,img_width, img_height, 3))\n",
    "\n",
    "    # Make prediction\n",
    "    try:\n",
    "        prediction = model.predict(features)\n",
    "    except:\n",
    "        prediction = model.predict(features.reshape(1, 7*7*512))\n",
    "        \n",
    "    classes = [\"with_mask\",\"without_mask\"]\n",
    "    print(\"I see...\"+str(classes[np.argmax(np.array(prediction[0]))]))\n",
    "pred_dir = \"/kaggle/input/intel-image-classification/seg_pred/seg_pred/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8221e2c0925c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpred_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "pred_dir = \"\"\n",
    "import random\n",
    "pred_files = random.sample(os.listdir(pred_dir),10)\n",
    "for f in pred_files:\n",
    "    prediction(pred_dir+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
